{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import geopandas as gpd\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\postv\\AppData\\Local\\Temp\\ipykernel_11440\\2455432584.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_data = pd.read_csv('dataset/alles_20171819_3tracties.csv', delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   ID  tractie maand       datum              tijdstip   \n",
      "0  427120170501070012     4271   mei  2017-05-01  2017-05-01T07:00:12Z  \\\n",
      "1  427120170501070017     4271   mei  2017-05-01  2017-05-01T07:00:17Z   \n",
      "2  427120170501070018     4271   mei  2017-05-01  2017-05-01T07:00:18Z   \n",
      "3  427120170501070019     4271   mei  2017-05-01  2017-05-01T07:00:19Z   \n",
      "4  427120170501070020     4271   mei  2017-05-01  2017-05-01T07:00:20Z   \n",
      "\n",
      "    actie       lat     long  seconde UTRGRID100  \n",
      "0  Rijden  52,10771  5,07925        1  1339_4577  \n",
      "1  Rijden  52,10771  5,07926        1  1339_4577  \n",
      "2  Rijden  52,10771  5,07928        1  1339_4577  \n",
      "3  Rijden  52,10771  5,07930        1  1339_4577  \n",
      "4  Rijden  52,10771  5,07932        1  1339_4577  \n",
      "ID            0\n",
      "tractie       0\n",
      "maand         0\n",
      "datum         0\n",
      "tijdstip      0\n",
      "actie         0\n",
      "lat           0\n",
      "long          0\n",
      "seconde       0\n",
      "UTRGRID100    0\n",
      "dtype: int64\n",
      "ID            object\n",
      "tractie        int64\n",
      "maand         object\n",
      "datum         object\n",
      "tijdstip      object\n",
      "actie         object\n",
      "lat           object\n",
      "long          object\n",
      "seconde        int64\n",
      "UTRGRID100    object\n",
      "dtype: object\n",
      "            tractie     seconde\n",
      "count  2.347336e+07  23473356.0\n",
      "mean   4.281485e+03         1.0\n",
      "std    7.124604e+00         0.0\n",
      "min    4.271000e+03         1.0\n",
      "25%    4.271000e+03         1.0\n",
      "50%    4.284000e+03         1.0\n",
      "75%    4.288000e+03         1.0\n",
      "max    4.288000e+03         1.0\n"
     ]
    }
   ],
   "source": [
    "csv_data = pd.read_csv('dataset/alles_20171819_3tracties.csv', delimiter=';')\n",
    "print(csv_data.head())\n",
    "print(csv_data.isnull().sum())\n",
    "print(csv_data.dtypes)\n",
    "print(csv_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID  Geometry_A  Geometry_P UTRGRID100      XCEN      XMAX      XMIN   \n",
      "0  11186.0     10000.0       400.0  1289_4559  128950.0  129000.0  128900.0  \\\n",
      "1  16061.0     10000.0       400.0  1359_4590  135950.0  136000.0  135900.0   \n",
      "2     61.0     10000.0       400.0  1324_4487  132450.0  132500.0  132400.0   \n",
      "3  11187.0     10000.0       400.0  1290_4559  129050.0  129100.0  129000.0   \n",
      "4  16062.0     10000.0       400.0  1360_4590  136050.0  136100.0  136000.0   \n",
      "\n",
      "       YCEN      YMAX      YMIN   \n",
      "0  455950.0  456000.0  455900.0  \\\n",
      "1  459050.0  459100.0  459000.0   \n",
      "2  448750.0  448800.0  448700.0   \n",
      "3  455950.0  456000.0  455900.0   \n",
      "4  459050.0  459100.0  459000.0   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((5.00636 52.09074, 5.00636 52.09164, ...  \n",
      "1  POLYGON ((5.10833 52.11889, 5.10832 52.11979, ...  \n",
      "2  POLYGON ((5.05791 52.02618, 5.0579 52.02708, 5...  \n",
      "3  POLYGON ((5.00782 52.09075, 5.00781 52.09165, ...  \n",
      "4  POLYGON ((5.10979 52.11889, 5.10978 52.11979, ...  \n",
      "Index(['ID', 'Geometry_A', 'Geometry_P', 'UTRGRID100', 'XCEN', 'XMAX', 'XMIN',\n",
      "       'YCEN', 'YMAX', 'YMIN', 'geometry'],\n",
      "      dtype='object')\n",
      "EPSG:4326\n"
     ]
    }
   ],
   "source": [
    "grid_data = gpd.read_file(\"dataset/UTRGRID100/UTRGRID100WGS84.shp\")\n",
    "grid_data = grid_data.to_crs(epsg=4326)\n",
    "print(grid_data.head())\n",
    "print(grid_data.columns)\n",
    "print(grid_data.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering for all models and summary of model requirements\n",
    " - aggregate data bygrid and date. We need the total sweeping time per grid per day. \n",
    " - include some lagged features for regression / rnn to be useful\n",
    " - add rolling averages\n",
    " - add Temporal features like day, week, month\n",
    " - add categorical features for xgb \n",
    "\n",
    "\n",
    " 1. Regression model\n",
    "    - Target: seconde (continuous variable).\n",
    "    - Features: Use all lagged, rolling, and temporal features.\n",
    " 2. RNN\n",
    "    - Group the dataset by UTRGRID100 and create sequences of input-output pairs:\n",
    "    - Input: Lagged and rolling features.\n",
    "    - Output: seconde (future sweeping effort).\n",
    " 3.\n",
    "    - Features: Lagged, rolling, and temporal features.\n",
    "    - Target: hotspot_category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_data['tijdstip'] = pd.to_datetime(csv_data['tijdstip'])\n",
    "csv_data['datum'] = pd.to_datetime(csv_data['datum'])\n",
    "\n",
    "# filter for arbeid and grid/date\n",
    "arbeid_data = csv_data[csv_data['actie'] == 'Arbeid']\n",
    "aggregated_data = arbeid_data.groupby(['datum', 'UTRGRID100']).agg({\n",
    "    'seconde': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# lagged features\n",
    "aggregated_data['lagged_seconde_1'] = aggregated_data.groupby('UTRGRID100')['seconde'].shift(1)\n",
    "aggregated_data['lagged_seconde_2'] = aggregated_data.groupby('UTRGRID100')['seconde'].shift(2)\n",
    "\n",
    "# rolling average\n",
    "aggregated_data['rolling_mean_seconde_3'] = aggregated_data.groupby('UTRGRID100')['seconde'] \\\n",
    "    .rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "aggregated_data['rolling_mean_seconde_5'] = aggregated_data.groupby('UTRGRID100')['seconde'] \\\n",
    "    .rolling(window=5, min_periods=1).mean().reset_index(0, drop=True)\n",
    "# temporal features\n",
    "aggregated_data['day_of_week'] = aggregated_data['datum'].dt.weekday\n",
    "aggregated_data['month'] = aggregated_data['datum'].dt.month\n",
    "aggregated_data['week_of_year'] = aggregated_data['datum'].dt.isocalendar().week\n",
    "\n",
    "# hotspot category\n",
    "aggregated_data['hotspot_category'] = pd.cut(\n",
    "    aggregated_data['seconde'] / 60,\n",
    "    bins=[0, 5, 10, 15, float('inf')],\n",
    "    labels=[\"<5 min\", \"5-10 min\", \"10-15 min\", \"15+ min\"]\n",
    ")\n",
    "\n",
    "aggregated_data['lagged_seconde_1'].fillna(0, inplace=True)\n",
    "aggregated_data['lagged_seconde_2'].fillna(0, inplace=True)\n",
    "\n",
    "# merging the shapefiles with the adjusted features\n",
    "# doing this before due to computational resources\n",
    "# although could try do it after since some spatial relationships are lost\n",
    "data = grid_data.merge(aggregated_data, on='UTRGRID100', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID  Geometry_A  Geometry_P UTRGRID100      XCEN      XMAX      XMIN   \n",
      "0  10928.0     10000.0       400.0  1341_4557  134150.0  134200.0  134100.0  \\\n",
      "1  10928.0     10000.0       400.0  1341_4557  134150.0  134200.0  134100.0   \n",
      "2  11083.0     10000.0       400.0  1341_4558  134150.0  134200.0  134100.0   \n",
      "3  10788.0     10000.0       400.0  1356_4556  135650.0  135700.0  135600.0   \n",
      "4  10789.0     10000.0       400.0  1357_4556  135750.0  135800.0  135700.0   \n",
      "\n",
      "       YCEN      YMAX      YMIN  ...      datum seconde  lagged_seconde_1   \n",
      "0  455750.0  455800.0  455700.0  ... 2017-05-18       4               0.0  \\\n",
      "1  455750.0  455800.0  455700.0  ... 2018-07-06       2               4.0   \n",
      "2  455850.0  455900.0  455800.0  ... 2018-10-19       4               0.0   \n",
      "3  455650.0  455700.0  455600.0  ... 2019-04-10      23               0.0   \n",
      "4  455650.0  455700.0  455600.0  ... 2019-04-10      68               0.0   \n",
      "\n",
      "   lagged_seconde_2  rolling_mean_seconde_3  rolling_mean_seconde_5   \n",
      "0               0.0                     4.0                     4.0  \\\n",
      "1               0.0                     3.0                     3.0   \n",
      "2               0.0                     4.0                     4.0   \n",
      "3               0.0                    23.0                    23.0   \n",
      "4               0.0                    68.0                    68.0   \n",
      "\n",
      "   day_of_week  month  week_of_year  hotspot_category  \n",
      "0            3      5            20            <5 min  \n",
      "1            4      7            27            <5 min  \n",
      "2            4     10            42            <5 min  \n",
      "3            2      4            15            <5 min  \n",
      "4            2      4            15            <5 min  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "datum                     0\n",
      "UTRGRID100                0\n",
      "seconde                   0\n",
      "lagged_seconde_1          0\n",
      "lagged_seconde_2          0\n",
      "rolling_mean_seconde_3    0\n",
      "rolling_mean_seconde_5    0\n",
      "day_of_week               0\n",
      "month                     0\n",
      "week_of_year              0\n",
      "hotspot_category          0\n",
      "dtype: int64\n",
      "                               datum        seconde  lagged_seconde_1   \n",
      "count                         119912  119912.000000     119912.000000  \\\n",
      "mean   2018-10-09 18:44:08.662352384     152.606370        151.176988   \n",
      "min              2017-05-01 00:00:00       1.000000          0.000000   \n",
      "25%              2018-03-07 00:00:00      41.000000         40.000000   \n",
      "50%              2018-10-26 00:00:00      91.000000         90.000000   \n",
      "75%              2019-05-28 00:00:00     184.000000        183.000000   \n",
      "max              2019-12-31 00:00:00    4536.000000       4536.000000   \n",
      "std                              NaN     201.841013        201.114308   \n",
      "\n",
      "       lagged_seconde_2  rolling_mean_seconde_3  rolling_mean_seconde_5   \n",
      "count     119912.000000           119912.000000           119912.000000  \\\n",
      "mean         149.789938              152.821468              152.926381   \n",
      "min            0.000000                1.000000                1.000000   \n",
      "25%           38.000000               63.333333               69.600000   \n",
      "50%           88.000000              110.666667              115.800000   \n",
      "75%          182.000000              190.666667              191.400000   \n",
      "max         4536.000000             2865.000000             2311.600000   \n",
      "std          200.567975              149.154128              137.336190   \n",
      "\n",
      "         day_of_week          month  week_of_year  \n",
      "count  119912.000000  119912.000000      119912.0  \n",
      "mean        1.927297       6.939714     28.272633  \n",
      "min         0.000000       1.000000           1.0  \n",
      "25%         1.000000       4.000000          16.0  \n",
      "50%         2.000000       7.000000          29.0  \n",
      "75%         3.000000      10.000000          41.0  \n",
      "max         6.000000      12.000000          52.0  \n",
      "std         1.438084       3.314449     14.495173  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "print(aggregated_data.isnull().sum())\n",
    "print(aggregated_data.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical XGBoost\n",
    "Input features:\n",
    " - Lagged features \n",
    " - Rolling averages\n",
    " - Temporal features\n",
    " - Categorical features\n",
    "\n",
    "Output features:\n",
    " - Hotspot category (4 categories)\n",
    "    - <5 min\n",
    "    - 5-10 min\n",
    "    - 10-15 min\n",
    "    - 15+ min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 0.2860572042753926, 1: 2.740931456688408, 2: 11.213951310861423, 3: 19.927620632279535}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\postv\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [15:47:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     20934\n",
      "           1       0.94      0.94      0.94      2185\n",
      "           2       0.88      0.84      0.86       534\n",
      "           3       0.86      0.90      0.88       300\n",
      "\n",
      "    accuracy                           0.99     23953\n",
      "   macro avg       0.92      0.92      0.92     23953\n",
      "weighted avg       0.99      0.99      0.99     23953\n",
      "\n",
      "Accuracy: 0.9854298000250491\n",
      "Balanced Accuracy: 0.9188401754229489\n"
     ]
    }
   ],
   "source": [
    "# input / output features\n",
    "features = [\n",
    "    'lagged_seconde_1', \n",
    "    'lagged_seconde_2',\n",
    "    'rolling_mean_seconde_3', \n",
    "    'rolling_mean_seconde_5',\n",
    "    'day_of_week', \n",
    "    'month', \n",
    "    'week_of_year'\n",
    "]\n",
    "target = 'hotspot_category'\n",
    "data[target] = data[target].astype('category').cat.codes\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# 80/20 split to evaluate initial performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# since there is a huge class imbalance im adjusting weights\n",
    "class_weights = {i: len(y_train) / (len(y_train.unique()) * count) \n",
    "                 for i, count in y_train.value_counts().items()}\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "# fit model\n",
    "model = xgb.XGBClassifier(\n",
    "    eval_metric='mlogloss',  \n",
    "    use_label_encoder=False, \n",
    "    random_state=42,\n",
    "    scale_pos_weight=[class_weights[i] for i in range(len(class_weights))]\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest regression\n",
    "Input features:\n",
    " - Lagged features \n",
    " - Rolling averages\n",
    " - Temporal features\n",
    "\n",
    "Output features:\n",
    " - Seconde (continuous)\n",
    "    - Total sweeping time (in seconds) for a specific grid (UTRGRID100) on a given day (datum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 5.4878123825825575\n",
      "Mean Squared Error (MSE): 653.2276296956541\n",
      "Root Mean Squared Error (RMSE): 25.558318209452946\n",
      "R² Score: 0.9834148857972359\n"
     ]
    }
   ],
   "source": [
    "# input / output features\n",
    "features = [\n",
    "    'lagged_seconde_1', \n",
    "    'lagged_seconde_2',\n",
    "    'rolling_mean_seconde_3', \n",
    "    'rolling_mean_seconde_5',\n",
    "    'day_of_week', \n",
    "    'month', \n",
    "    'week_of_year'\n",
    "]\n",
    "target = 'seconde'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# 80/20 split to evaluate initial performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# train model\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators = 100,\n",
    "    random_state = 42,\n",
    "    n_jobs = -1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent neural network (RNN)\n",
    "Due to the sequential nature of the data i will as an attempt to exploration implement LSTM layers inside this RNN.\n",
    "Input features:\n",
    " - Lagged features \n",
    " - Rolling averages\n",
    " - Temporal features\n",
    "\n",
    "Output features:\n",
    " - Seconde (continuous)\n",
    "    - Total sweeping time (in seconds) for a specific grid (UTRGRID100) on a given day (datum).\n",
    "\n",
    "Architecture:\n",
    " - Input: A sequence of feature vectors that includes lagged, rolling, and temporal data for each grid and day.\n",
    " - LSTM Layer: Captures the dependencies and patterns in the time-series data.\n",
    " - Fully Connected Layer: Converts the LSTM outputs into a predicted value.\n",
    " - Output: The model outputs the predicted sweeping time (seconde), a continuous value representing the effort required for a grid on a given day.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Loss: 3.8209400177001953\n",
      "Epoch [10/100], Loss: 3.7591395378112793\n",
      "Epoch [15/100], Loss: 3.692800283432007\n",
      "Epoch [20/100], Loss: 3.6150896549224854\n",
      "Epoch [25/100], Loss: 3.5124378204345703\n",
      "Epoch [30/100], Loss: 3.3558759689331055\n",
      "Epoch [35/100], Loss: 3.0915231704711914\n",
      "Epoch [40/100], Loss: 2.703521728515625\n",
      "Epoch [45/100], Loss: 2.2903547286987305\n",
      "Epoch [50/100], Loss: 1.9132368564605713\n",
      "Epoch [55/100], Loss: 1.5695843696594238\n",
      "Epoch [60/100], Loss: 1.2581634521484375\n",
      "Epoch [65/100], Loss: 0.9945753812789917\n",
      "Epoch [70/100], Loss: 0.7917879223823547\n",
      "Epoch [75/100], Loss: 0.6548727750778198\n",
      "Epoch [80/100], Loss: 0.5799463987350464\n",
      "Epoch [85/100], Loss: 0.5511438250541687\n",
      "Epoch [90/100], Loss: 0.5470896363258362\n",
      "Epoch [95/100], Loss: 0.5506176948547363\n",
      "Epoch [100/100], Loss: 0.5529188513755798\n",
      "Mean Absolute Error (MAE): 110.6532211303711\n",
      "Mean Squared Error (MSE): 42800.203125\n",
      "Root Mean Squared Error (RMSE): 206.88209533691406\n",
      "R² Score: -0.045228204404368144\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    'lagged_seconde_1', \n",
    "    'lagged_seconde_2',\n",
    "    'rolling_mean_seconde_3', \n",
    "    'rolling_mean_seconde_5',\n",
    "    'day_of_week', \n",
    "    'month', \n",
    "    'week_of_year'\n",
    "]\n",
    "target = 'seconde'\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(data[features])\n",
    "\n",
    "# applying log transformation to the target to make the distribution more manageable\n",
    "# this makes the variance more stable\n",
    "data[target] = np.log1p(data[target])\n",
    "\n",
    "# create lstm sequences\n",
    "# they determine what days are taken with the input of the lstm\n",
    "# memory of 3 days is tested\n",
    "def create_sequences(X, y, seq_length=20):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i + seq_length])\n",
    "        y_seq.append(y[i + seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_scaled, data[target].values)\n",
    "\n",
    "# train test split with tensors\n",
    "X_seq_tensor = torch.tensor(X_seq, dtype=torch.float32)\n",
    "y_seq_tensor = torch.tensor(y_seq, dtype=torch.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq_tensor, y_seq_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# bidirectional lstm model for testing\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)  # BiLSTM\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # Multiply by 2 because of bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, (hn, cn) = self.bilstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Using the last hidden state for output\n",
    "        return out\n",
    "\n",
    "# initialize model further\n",
    "input_size = X_train.shape[2]  \n",
    "hidden_size = 32  \n",
    "output_size = 1  \n",
    "model = LSTM(input_size, hidden_size, output_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "# train model\n",
    "epochs = 100 \n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred.squeeze(), y_train)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    y_pred = model(X_test)\n",
    "    y_pred = np.expm1(y_pred.numpy()) \n",
    "    \n",
    "    y_test = np.expm1(y_test.numpy())\n",
    "\n",
    "# calculate metrics\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "print(f\"R² Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
